{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c33b7875",
   "metadata": {},
   "source": [
    "# Redes Neurais com PyTorch\n",
    "\n",
    "Este notebook serve como um guia introdutório ao desenvolvimento de redes neurais utilizando a biblioteca PyTorch. Abordaremos os componentes fundamentais da biblioteca, desde a manipulação de tensores até a construção e o treinamento de um modelo de rede neural para classificação de imagens.\n",
    "\n",
    "## Conteúdos Abordados\n",
    "\n",
    "1.  **Tensores e Grafos Computacionais**: A base do PyTorch.\n",
    "2.  **O Módulo `torch.nn`**: Construindo camadas da rede.\n",
    "3.  **Funções de Ativação**: Introduzindo não linearidade.\n",
    "4.  **Construindo Modelos**: `nn.Sequential` e classes customizadas com `nn.Module`.\n",
    "5.  **Datasets e DataLoaders**: Gerenciando e preparando dados com o `torchvision`.\n",
    "6.  **Funções de Custo (Loss Functions)**: Quantificando o erro do modelo.\n",
    "7.  **Otimizadores**: Atualizando os pesos do modelo.\n",
    "8.  **Treinamento**: O ciclo completo de forward, backward e otimização."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f64f89",
   "metadata": {},
   "source": [
    "## 1. Tensores e Grafos Computacionais\n",
    "\n",
    "O `Tensor` é a estrutura de dados central do PyTorch. Trata-se de uma matriz multidimensional otimizada para operações em hardware especializado como GPUs. Os tensores são a base para a construção de grafos computacionais dinâmicos, que são essenciais para o cálculo automático de gradientes através do mecanismo de diferenciação automática, conhecido como `autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a888fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Criando um tensor a partir de uma lista Python\n",
    "data_list = [[1, 2], [3, 4]]\n",
    "x_data = torch.tensor(data_list, dtype=torch.float32)\n",
    "print(f\"Tensor a partir de lista:\\n {x_data}\\n\")\n",
    "\n",
    "# Funções de criação de tensores\n",
    "x_ones = torch.ones_like(x_data) # Cria um tensor de 'uns' com o mesmo formato de x_data\n",
    "print(f\"Tensor de 'uns':\\n {x_ones} \\n\")\n",
    "\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float) # Cria um tensor com valores aleatórios\n",
    "print(f\"Tensor aleatório:\\n {x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8909834a",
   "metadata": {},
   "source": [
    "### O Grafo Computacional e o `requires_grad`\n",
    "\n",
    "Cada tensor possui um atributo booleano chamado `requires_grad`. Se definido como `True`, o `autograd` rastreia todas as operações realizadas neste tensor. Ao final de uma cadeia de operações (tipicamente, o cálculo de uma função de custo), a chamada do método `.backward()` no tensor de saída (que deve ser um escalar) dispara o cálculo dos gradientes de forma retroativa através do grafo, aplicando a regra da cadeia (chain rule).\n",
    "\n",
    "Considere a seguinte função, onde $a$, $b$, e $c$ são tensores de entrada:\n",
    "$$\n",
    "L = (a \\cdot b - c)^2\n",
    "$$\n",
    "O `autograd` precisa calcular as derivadas parciais de $L$ em relação a $a$, $b$ e $c$, ou seja, $\\frac{\\partial L}{\\partial a}$, $\\frac{\\partial L}{\\partial b}$, e $\\frac{\\partial L}{\\partial c}$. Para calcular $\\frac{\\partial L}{\\partial a}$, por exemplo, a regra da cadeia se desdobra da seguinte forma, sendo $z = a \\cdot b - c$:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial a} = \\frac{\\partial L}{\\partial z} \\cdot \\frac{\\partial z}{\\partial a}\n",
    "$$\n",
    "Onde $\\frac{\\partial L}{\\partial z} = 2z = 2(a \\cdot b - c)$ e $\\frac{\\partial z}{\\partial a} = b$. O PyTorch realiza este processo de forma automática para todos os parâmetros com `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3080aafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementando o exemplo matemático com autograd\n",
    "# Definindo os tensores de entrada que requerem gradiente\n",
    "a = torch.tensor(4.0, requires_grad=True)\n",
    "b = torch.tensor(5.0, requires_grad=True)\n",
    "c = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# Sequência de operações\n",
    "z = a * b - c\n",
    "L = z**2\n",
    "\n",
    "# O tensor 'L' agora possui uma função de gradiente que armazena o grafo\n",
    "print(f\"Tensor de saída L: {L}\")\n",
    "print(f\"Função de gradiente de L: {L.grad_fn}\\n\")\n",
    "\n",
    "# Calculando os gradientes (backward pass)\n",
    "L.backward()\n",
    "\n",
    "# Os gradientes são acumulados no atributo .grad de cada tensor\n",
    "print(f\"Gradiente dL/da: {a.grad}\") # Esperado: 2 * (4*5 - 2) * 5 = 180\n",
    "print(f\"Gradiente dL/db: {b.grad}\") # Esperado: 2 * (4*5 - 2) * 4 = 144\n",
    "print(f\"Gradiente dL/dc: {c.grad}\") # Esperado: 2 * (4*5 - 2) * (-1) = -36"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ec41d9",
   "metadata": {},
   "source": [
    "## 2. O Módulo `torch.nn`\n",
    "\n",
    "O `torch.nn` é o módulo do PyTorch para a construção de redes neurais. Ele fornece um conjunto de blocos de construção, como camadas (`Layers`), funções de ativação (`Activation Functions`), funções de custo (`Loss Functions`) e contêineres (`Containers`). Uma \"camada\" no `torch.nn` é um objeto que encapsula tanto os pesos (parâmetros) quanto as operações a serem aplicadas nos dados de entrada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2222abd9",
   "metadata": {},
   "source": [
    "### Camada Linear: `nn.Linear`\n",
    "\n",
    "A camada mais fundamental é a `nn.Linear`, que aplica uma transformação afim aos dados de entrada: $y = xW^T + b$.\n",
    "\n",
    "-   `in_features`: a dimensionalidade do espaço de entrada.\n",
    "-   `out_features`: a dimensionalidade do espaço de saída.\n",
    "\n",
    "Os tensores de peso (`weight`, $W$) e de viés (`bias`, $b$) são encapsulados como `nn.Parameter`, uma subclasse de `torch.Tensor` que os registra automaticamente como parâmetros de um `nn.Module`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ee0e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Definindo uma camada linear\n",
    "# Entrada: 10 features\n",
    "# Saída: 5 features\n",
    "linear_layer = nn.Linear(in_features=10, out_features=5)\n",
    "\n",
    "# Criando um tensor de entrada de exemplo (um \"batch\" com 3 amostras)\n",
    "# O formato é (batch_size, in_features)\n",
    "input_tensor = torch.randn(3, 10)\n",
    "\n",
    "# Passando os dados pela camada\n",
    "output_tensor = linear_layer(input_tensor)\n",
    "\n",
    "print(f\"Formato do tensor de entrada: {input_tensor.shape}\")\n",
    "print(f\"Formato do tensor de saída: {output_tensor.shape}\")\n",
    "print(f\"\\nPesos (weights) da camada:\\n {linear_layer.weight.shape}\")\n",
    "print(f\"Viés (bias) da camada:\\n {linear_layer.bias.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db32237",
   "metadata": {},
   "source": [
    "## 3. Funções de Ativação\n",
    "\n",
    "Funções de ativação introduzem não linearidade no modelo, capacitando-o a aprender fronteiras de decisão complexas. Elas são aplicadas elemento a elemento na saída de uma camada.\n",
    "\n",
    "### Funções Comuns\n",
    "\n",
    "-   **Sigmoid**: Comprime os valores de entrada no intervalo $(0, 1)$. Utilizada historicamente em camadas ocultas e atualmente em camadas de saída para classificação binária.\n",
    "    $$ \\sigma(x) = \\frac{1}{1 + e^{-x}} $$\n",
    "-   **Tanh (Tangente Hiperbólica)**: Comprime os valores de entrada no intervalo $(-1, 1)$. Geralmente converge mais rápido que a Sigmoid por ser centrada em zero.\n",
    "    $$ \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}} $$\n",
    "-   **ReLU (Rectified Linear Unit)**: A função de ativação mais utilizada em redes profundas. É computacionalmente eficiente e ajuda a mitigar o problema do desaparecimento do gradiente (vanishing gradient).\n",
    "    $$ \\text{ReLU}(x) = \\max(0, x) $$\n",
    "-   **Leaky ReLU**: Uma variação da ReLU que permite a passagem de um pequeno gradiente negativo, prevenindo o problema dos \"neurônios mortos\".\n",
    "    $$ \\text{LeakyReLU}(x) = \\begin{cases} x, & \\text{if } x > 0 \\\\ \\alpha x, & \\text{otherwise} \\end{cases} $$\n",
    "-   **Softmax**: Transforma um vetor de números reais (logits) em uma distribuição de probabilidade sobre múltiplas classes. Utilizada na camada de saída para classificação multiclasse.\n",
    "    $$ \\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab214c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As funções de ativação também estão no módulo nn\n",
    "relu_activation = nn.ReLU()\n",
    "leaky_relu_activation = nn.LeakyReLU()\n",
    "\n",
    "# Aplicando a ativação na saída da camada linear anterior\n",
    "output_with_relu = relu_activation(output_tensor)\n",
    "output_with_leaky_relu = leaky_relu_activation(output_tensor)\n",
    "\n",
    "print(f\"Saída da camada linear:\\n {output_tensor}\\n\")\n",
    "print(f\"Saída após ativação ReLU:\\n {output_with_relu}\\n\")\n",
    "print(f\"Saída após ativação Leaky ReLU:\\n {output_with_leaky_relu}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c7291d",
   "metadata": {},
   "source": [
    "## 4. Construindo Modelos\n",
    "\n",
    "O PyTorch oferece duas maneiras principais de agrupar camadas para formar um modelo completo.\n",
    "\n",
    "### `nn.Sequential`\n",
    "\n",
    "`nn.Sequential` é um contêiner que recebe uma sequência de módulos (camadas, funções de ativação, etc.) e os executa na ordem em que são passados. É uma forma rápida e simples de criar modelos onde os dados fluem sequencialmente através das camadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965dde91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construindo um modelo simples com nn.Sequential\n",
    "# Input (784) -> Linear (128) -> ReLU -> Linear (10) -> Output\n",
    "model_sequential = nn.Sequential(\n",
    "    nn.Linear(in_features=784, out_features=128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=128, out_features=10)\n",
    ")\n",
    "\n",
    "# Criando um tensor de entrada de exemplo (batch com 64 imagens achatadas de 28x28)\n",
    "# 28 * 28 = 784\n",
    "input_images = torch.randn(64, 784)\n",
    "\n",
    "# Forward pass através do modelo sequencial\n",
    "logits = model_sequential(input_images) # 'logits' são as saídas brutas antes da probabilidade\n",
    "\n",
    "print(f\"Formato da saída do modelo: {logits.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c230087",
   "metadata": {},
   "source": [
    "### Classes customizadas com `nn.Module`\n",
    "\n",
    "Para modelos mais complexos, como aqueles com múltiplos caminhos de entrada/saída ou lógicas de *forward pass* não sequenciais (e.g., redes residuais), a abordagem recomendada é criar uma classe que herda de `nn.Module`.\n",
    "\n",
    "Toda classe de modelo customizada deve:\n",
    "1.  Herdar de `torch.nn.Module`.\n",
    "2.  Definir as camadas no construtor `__init__(self)`.\n",
    "3.  Implementar a lógica do *forward pass* no método `forward(self, x)`.\n",
    "\n",
    "O método `backward()` é gerenciado automaticamente pelo `autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2492bef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definindo o mesmo modelo anterior, mas agora como uma classe\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.layer2(out)\n",
    "        return out\n",
    "\n",
    "# Instanciando o modelo\n",
    "model_class = NeuralNetwork(input_size=784, hidden_size=128, num_classes=10)\n",
    "\n",
    "# O uso é idêntico\n",
    "logits_class = model_class(input_images)\n",
    "\n",
    "print(f\"Formato da saída do modelo (classe): {logits_class.shape}\")\n",
    "print(f\"\\nEstrutura do modelo:\\n{model_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e0aed0",
   "metadata": {},
   "source": [
    "## 5. Datasets e DataLoaders\n",
    "\n",
    "Para treinar um modelo, precisamos de dados. O `torch.utils.data.Dataset` é uma classe abstrata que representa um dataset. O `torch.utils.data.DataLoader` é um iterador que envolve um `Dataset` e oferece funcionalidades como:\n",
    "\n",
    "-   Agrupamento dos dados em *batches* (lotes).\n",
    "-   Embaralhamento dos dados a cada época (`shuffle=True`).\n",
    "-   Carregamento de dados em paralelo usando múltiplos processos (`num_workers`).\n",
    "\n",
    "Um **batch** é um subconjunto do dataset total. Em vez de calcular o gradiente com base em todo o dataset de uma só vez (o que seria computacionalmente caro), o treinamento é feito iterativamente em batches. Isso torna o processo de treinamento mais eficiente e pode levar a uma convergência mais estável.\n",
    "\n",
    "A biblioteca `torchvision` contém datasets populares (como o MNIST), modelos pré-treinados e transformações de imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40c9ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Definindo transformações para os dados\n",
    "# ToTensor() converte a imagem PIL (H x W x C) no intervalo [0, 255]\n",
    "# para um FloatTensor (C x H x W) no intervalo [0.0, 1.0].\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # Média e desvio padrão do MNIST\n",
    "])\n",
    "\n",
    "# Baixando o dataset de treino\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    transform=transform,\n",
    "    download=True\n",
    ")\n",
    "\n",
    "# Baixando o dataset de teste\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad79391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando os DataLoaders\n",
    "batch_size = 100\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# Iterando sobre o DataLoader para ver o formato de um batch\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"Formato do batch de imagens: {images.shape}\") # (batch_size, channels, height, width)\n",
    "print(f\"Formato do batch de rótulos: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9b6e42",
   "metadata": {},
   "source": [
    "## 6. Funções de Custo (Loss Functions)\n",
    "\n",
    "A função de custo $J(\\theta)$ mede a discrepância entre a saída prevista pelo modelo $\\hat{y}$ e o valor real $y$. O objetivo do treinamento é encontrar os parâmetros $\\theta$ que minimizam $J(\\theta)$.\n",
    "\n",
    "### Funções Comuns\n",
    "\n",
    "-   **Mean Squared Error (MSE)**: Utilizada principalmente para tarefas de regressão. Calcula a média dos erros quadráticos entre a previsão e o valor real.\n",
    "    $$ J_{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2 $$\n",
    "-   **Binary Cross-Entropy (BCE)**: Utilizada para classificação binária. Geralmente é combinada com uma camada de saída Sigmoid.\n",
    "    $$ J_{BCE} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] $$\n",
    "-   **Cross-Entropy Loss**: Utilizada para classificação multiclasse. No PyTorch, `nn.CrossEntropyLoss` combina `nn.LogSoftmax` e `nn.NLLLoss`, sendo numericamente mais estável. Ela espera como entrada os *logits* brutos do modelo e os rótulos de classe como inteiros.\n",
    "    $$ J_{CE} = -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{p}_{i,c}) $$\n",
    "    Onde $y_{i,c}$ é 1 se a amostra $i$ pertence à classe $c$ (0 caso contrário), e $\\hat{p}_{i,c}$ é a probabilidade prevista pelo modelo para a amostra $i$ pertencer à classe $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc54866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando a função de custo para classificação multiclasse\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "# Exemplo de uso:\n",
    "# Saída do modelo (logits) para um batch de 3 amostras e 10 classes\n",
    "output_logits = torch.randn(3, 10)\n",
    "# Rótulos verdadeiros\n",
    "target_labels = torch.tensor([1, 4, 9]) # Classe 1, Classe 4, Classe 9\n",
    "\n",
    "# Calculando a perda\n",
    "loss = loss_function(output_logits, target_labels)\n",
    "print(f\"Valor da perda (loss): {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7e0143",
   "metadata": {},
   "source": [
    "## 7. Otimizadores\n",
    "\n",
    "O otimizador implementa o algoritmo de atualização dos parâmetros do modelo, $\\theta$, com base nos gradientes da função de custo, $\\nabla_{\\theta} J(\\theta)$. O objetivo é convergir para um mínimo (local ou global) da função de custo.\n",
    "\n",
    "O algoritmo mais fundamental é o **Stochastic Gradient Descent (SGD)**. A regra de atualização para um parâmetro $\\theta$ no passo de tempo $t$ é definida como:\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\eta \\nabla_{\\theta} J(\\theta_t)\n",
    "$$\n",
    "onde $\\eta$ é a taxa de aprendizado (*learning rate*), um hiperparâmetro que controla o tamanho do passo na direção do gradiente negativo.\n",
    "\n",
    "Otimizadores mais avançados, como o **Adam (Adaptive Moment Estimation)**, utilizam taxas de aprendizado adaptativas para cada parâmetro, mantendo uma estimativa do primeiro momento (a média) e do segundo momento (a variância não centrada) dos gradientes, o que frequentemente leva a uma convergência mais rápida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b6b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instanciando o modelo\n",
    "learning_rate = 0.001\n",
    "model = NeuralNetwork(input_size=784, hidden_size=128, num_classes=10)\n",
    "\n",
    "# Instanciando o otimizador Adam\n",
    "# Passamos os parâmetros do modelo que devem ser otimizados (model.parameters())\n",
    "# e a taxa de aprendizado (lr).\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# O otimizador possui métodos cruciais:\n",
    "# optimizer.zero_grad(): Zera os gradientes de todos os parâmetros antes de um novo cálculo de backward pass.\n",
    "#   Isto é necessário porque o método .backward() acumula os gradientes por padrão.\n",
    "#\n",
    "# optimizer.step(): Atualiza os parâmetros do modelo usando a lógica do otimizador (e.g., Adam) e os gradientes\n",
    "#   armazenados no atributo .grad de cada parâmetro."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02499521",
   "metadata": {},
   "source": [
    "## 8. Treinamento: O Ciclo Completo\n",
    "\n",
    "O treinamento de uma rede neural consiste em um loop que itera sobre o dataset por várias \"épocas\" (*epochs*). Uma época é uma passagem completa por todo o dataset de treinamento. Dentro de cada época, iteramos sobre os *batches* de dados.\n",
    "\n",
    "Para cada *batch*, o ciclo de treinamento é:\n",
    "1.  **Zerar os gradientes**: Chamar `optimizer.zero_grad()`.\n",
    "2.  **Forward Pass**: Passar os dados de entrada pelo modelo para obter as previsões (logits).\n",
    "3.  **Calcular a Perda**: Comparar as previsões com os rótulos verdadeiros usando a função de custo.\n",
    "4.  **Backward Pass**: Chamar `loss.backward()` para calcular os gradientes da perda em relação a cada parâmetro do modelo.\n",
    "5.  **Atualizar os Pesos**: Chamar `optimizer.step()` para que o otimizador atualize os pesos com base nos gradientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21914f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Hiperparâmetros\n",
    "num_epochs = 5\n",
    "input_size = 784 # 28x28\n",
    "hidden_size = 500\n",
    "num_classes = 10\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Dispositivo (GPU se disponível, senão CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando o dispositivo: {device}\")\n",
    "\n",
    "# Instanciando o modelo, função de custo e otimizador e movendo o modelo para o dispositivo\n",
    "model = NeuralNetwork(input_size, hidden_size, num_classes).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Listas para armazenar as métricas de cada época\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Loop de treinamento\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Treinamento ---\n",
    "    model.train() # Coloca o modelo em modo de treinamento\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "    progress_bar_train = tqdm(train_loader, desc=f'Época [{epoch+1}/{num_epochs}] Treino')\n",
    "    \n",
    "    for images, labels in progress_bar_train:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward e otimização\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Calcula a acurácia de treino\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        \n",
    "        progress_bar_train.set_postfix({'Perda Treino': f'{loss.item():.4f}'})\n",
    "        \n",
    "    epoch_train_loss = running_loss / len(train_loader)\n",
    "    epoch_train_acc = 100 * correct_train / total_train\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_accuracies.append(epoch_train_acc)\n",
    "\n",
    "    # --- Validação ---\n",
    "    model.eval() # Coloca o modelo em modo de avaliação\n",
    "    running_val_loss = 0.0\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    \n",
    "    progress_bar_val = tqdm(test_loader, desc=f'Época [{epoch+1}/{num_epochs}] Validação')\n",
    "    \n",
    "    with torch.no_grad(): # Desabilita o cálculo de gradientes\n",
    "        for images, labels in progress_bar_val:\n",
    "            images = images.reshape(-1, 28*28).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_val_loss += loss.item()\n",
    "            \n",
    "            # Calcula a acurácia de validação\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_val += labels.size(0)\n",
    "            correct_val += (predicted == labels).sum().item()\n",
    "            \n",
    "            progress_bar_val.set_postfix({'Perda Val': f'{loss.item():.4f}'})\n",
    "\n",
    "    epoch_val_loss = running_val_loss / len(test_loader)\n",
    "    epoch_val_acc = 100 * correct_val / total_val\n",
    "    val_losses.append(epoch_val_loss)\n",
    "    val_accuracies.append(epoch_val_acc)\n",
    "    \n",
    "    print(f'Fim da Época [{epoch+1}/{num_epochs}] | '\n",
    "          f'Perda Treino: {epoch_train_loss:.4f}, Acurácia Treino: {epoch_train_acc:.2f}% | '\n",
    "          f'Perda Validação: {epoch_val_loss:.4f}, Acurácia Validação: {epoch_val_acc:.2f}%')\n",
    "\n",
    "print(\"\\nTreinamento concluído!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2525033",
   "metadata": {},
   "source": [
    "### Visualização das Curvas de Aprendizado\n",
    "\n",
    "As curvas de aprendizado plotam as métricas de desempenho (como perda e acurácia) para os conjuntos de treinamento e validação ao longo das épocas. Elas são ferramentas de diagnóstico essenciais:\n",
    "\n",
    "-   **Curva de Perda (Loss Curve)**: Mostra a evolução da função de custo. Idealmente, ambas as perdas (treino e validação) devem diminuir. Se a perda de validação começar a aumentar enquanto a de treino continua caindo, é um sinal claro de *overfitting*.\n",
    "-   **Curva de Acurácia (Accuracy Curve)**: Mostra a evolução da acurácia. Idealmente, ambas devem aumentar e convergir. Uma grande diferença entre a acurácia de treino e a de validação também indica *overfitting*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a99a8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando os plots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Plot da Curva de Perda\n",
    "axs[0].plot(train_losses, label='Perda de Treino')\n",
    "axs[0].plot(val_losses, label='Perda de Validação')\n",
    "axs[0].set_title(\"Curvas de Perda\")\n",
    "axs[0].set_xlabel(\"Época\")\n",
    "axs[0].set_ylabel(\"Perda (Cross-Entropy Loss)\")\n",
    "axs[0].legend()\n",
    "axs[0].grid(True)\n",
    "\n",
    "# Plot da Curva de Acurácia\n",
    "axs[1].plot(train_accuracies, label='Acurácia de Treino')\n",
    "axs[1].plot(val_accuracies, label='Acurácia de Validação')\n",
    "axs[1].set_title(\"Curvas de Acurácia\")\n",
    "axs[1].set_xlabel(\"Época\")\n",
    "axs[1].set_ylabel(\"Acurácia (%)\")\n",
    "axs[1].legend()\n",
    "axs[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0b83e7",
   "metadata": {},
   "source": [
    "### Avaliação do Modelo\n",
    "\n",
    "Após o treinamento, é fundamental avaliar a performance do modelo em dados que ele nunca viu, ou seja, o conjunto de teste. Durante a avaliação, não precisamos calcular gradientes, o que economiza memória e computação. Para isso, usamos o contexto `torch.no_grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c09725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colocando o modelo em modo de avaliação (desativa camadas como Dropout, se houver)\n",
    "model.eval()\n",
    "\n",
    "# O contexto torch.no_grad() desabilita o cálculo de gradientes\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        \n",
    "        # A classe com o maior logit é a previsão\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Acurácia do modelo no dataset de teste: {100 * correct / total:.2f} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
