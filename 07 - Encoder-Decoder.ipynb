{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 - Encoder-Decoder com LSTM\n",
    "\n",
    "Neste notebook, vamos implementar um modelo encoder-decoder simples usando LSTM para tradução. Esta arquitetura é fundamental para muitas tarefas de processamento de linguagem natural e representa um marco importante no desenvolvimento de sistemas de tradução automática.\n",
    "\n",
    "## Objetivos de Aprendizado\n",
    "- Compreender a arquitetura encoder-decoder\n",
    "- Implementar LSTM para codificação e decodificação de sequências\n",
    "- Criar um modelo de tradução português-inglês\n",
    "- Visualizar o processo de aprendizado\n",
    "- Avaliar a qualidade das traduções\n",
    "- Entender as limitações e possíveis melhorias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Configurações para visualização\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Dispositivo utilizado: {device}')\n",
    "\n",
    "# Configurar seeds para reprodutibilidade\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Fundamentos Teóricos\n",
    "\n",
    "### Arquitetura Encoder-Decoder\n",
    "\n",
    "A arquitetura encoder-decoder consiste em dois componentes principais:\n",
    "\n",
    "**1. Encoder (Codificador):**\n",
    "- Processa a sequência de entrada $\\mathbf{x} = (x_1, x_2, ..., x_T)$\n",
    "- Produz uma representação de contexto fixo $\\mathbf{c}$\n",
    "- Matematicamente: $\\mathbf{h}_t^{enc} = f_{enc}(\\mathbf{x}_t, \\mathbf{h}_{t-1}^{enc})$\n",
    "- O contexto final: $\\mathbf{c} = \\mathbf{h}_T^{enc}$\n",
    "\n",
    "**2. Decoder (Decodificador):**\n",
    "- Gera a sequência de saída $\\mathbf{y} = (y_1, y_2, ..., y_{T'})$\n",
    "- Usa o contexto $\\mathbf{c}$ como estado inicial\n",
    "- Matematicamente: $\\mathbf{h}_t^{dec} = f_{dec}(\\mathbf{y}_{t-1}, \\mathbf{h}_{t-1}^{dec})$\n",
    "- Probabilidade de saída: $P(y_t | y_1, ..., y_{t-1}, \\mathbf{c}) = \\text{softmax}(\\mathbf{W}_o \\mathbf{h}_t^{dec} + \\mathbf{b}_o)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparação dos Dados\n",
    "\n",
    "Vamos criar um conjunto de dados simples para tradução português-inglês, focando em estruturas gramaticais básicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset expandido de tradução português-inglês\n",
    "translation_pairs = [\n",
    "    # Frases básicas com pronomes\n",
    "    ('eu sou estudante', 'i am student'),\n",
    "    ('ele gosta de livros', 'he likes books'),\n",
    "    ('ela come uma maçã', 'she eats an apple'),\n",
    "    ('nós vamos à escola', 'we go to school'),\n",
    "    ('eles têm um carro', 'they have a car'),\n",
    "    ('você é inteligente', 'you are smart'),\n",
    "    \n",
    "    # Ações cotidianas\n",
    "    ('eu bebo água', 'i drink water'),\n",
    "    ('ele lê um jornal', 'he reads a newspaper'),\n",
    "    ('ela escreve uma carta', 'she writes a letter'),\n",
    "    ('nós estudamos matemática', 'we study mathematics'),\n",
    "    ('eles jogam futebol', 'they play football'),\n",
    "    ('você trabalha muito', 'you work hard'),\n",
    "    \n",
    "    # Descrições e estados\n",
    "    ('eu moro aqui', 'i live here'),\n",
    "    ('ele canta bem', 'he sings well'),\n",
    "    ('ela dança bonito', 'she dances beautifully'),\n",
    "    ('nós somos felizes', 'we are happy'),\n",
    "    ('eles estão cansados', 'they are tired'),\n",
    "    ('você fala português', 'you speak portuguese'),\n",
    "    \n",
    "    # Frases com objetos\n",
    "    ('eu tenho um gato', 'i have a cat'),\n",
    "    ('ele compra pão', 'he buys bread'),\n",
    "    ('ela vê um filme', 'she watches a movie'),\n",
    "    ('nós ouvimos música', 'we listen to music'),\n",
    "    ('eles fazem comida', 'they make food'),\n",
    "    ('você quer café', 'you want coffee')\n",
    "]\n",
    "\n",
    "print(f'Dataset criado com {len(translation_pairs)} pares de tradução')\n",
    "print('\\nExemplos de tradução:')\n",
    "for i, (pt, en) in enumerate(translation_pairs[:5]):\n",
    "    print(f'{i+1:2d}. PT: \"{pt}\" → EN: \"{en}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    \"\"\"Classe para gerenciar vocabulário e conversões palavra-índice\"\"\"\n",
    "    \n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2idx = {'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, '<UNK>': 3}\n",
    "        self.idx2word = {0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: '<UNK>'}\n",
    "        self.word_count = {}\n",
    "        self.n_words = 4\n",
    "    \n",
    "    def add_sentence(self, sentence):\n",
    "        \"\"\"Adiciona todas as palavras de uma frase ao vocabulário\"\"\"\n",
    "        for word in sentence.strip().lower().split():\n",
    "            self.add_word(word)\n",
    "    \n",
    "    def add_word(self, word):\n",
    "        \"\"\"Adiciona uma palavra ao vocabulário\"\"\"\n",
    "        if word not in self.word2idx:\n",
    "            self.word2idx[word] = self.n_words\n",
    "            self.idx2word[self.n_words] = word\n",
    "            self.word_count[word] = 1\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word_count[word] += 1\n",
    "    \n",
    "    def sentence_to_indices(self, sentence):\n",
    "        \"\"\"Converte uma frase em uma lista de índices\"\"\"\n",
    "        return [self.word2idx.get(word.lower(), self.word2idx['<UNK>']) \n",
    "                for word in sentence.strip().split()]\n",
    "    \n",
    "    def indices_to_sentence(self, indices):\n",
    "        \"\"\"Converte uma lista de índices de volta para uma frase\"\"\"\n",
    "        words = []\n",
    "        for idx in indices:\n",
    "            if idx == self.word2idx['<EOS>']:\n",
    "                break\n",
    "            if idx != self.word2idx['<PAD>']:\n",
    "                words.append(self.idx2word[idx])\n",
    "        return ' '.join(words)\n",
    "\n",
    "# Criar vocabulários para português e inglês\n",
    "pt_vocab = Vocabulary('português')\n",
    "en_vocab = Vocabulary('inglês')\n",
    "\n",
    "# Construir vocabulários a partir dos dados\n",
    "for pt_sentence, en_sentence in translation_pairs:\n",
    "    pt_vocab.add_sentence(pt_sentence)\n",
    "    en_vocab.add_sentence(en_sentence)\n",
    "\n",
    "print(f'Vocabulário português: {pt_vocab.n_words} palavras únicas')\n",
    "print(f'Vocabulário inglês: {en_vocab.n_words} palavras únicas')\n",
    "print(f'\\nPalavras mais frequentes em português: {sorted(pt_vocab.word_count.items(), key=lambda x: x[1], reverse=True)[:10]}')\n",
    "print(f'Palavras mais frequentes em inglês: {sorted(en_vocab.word_count.items(), key=lambda x: x[1], reverse=True)[:10]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualização dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análise estatística dos dados\n",
    "pt_lengths = [len(pt.split()) for pt, _ in translation_pairs]\n",
    "en_lengths = [len(en.split()) for _, en in translation_pairs]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Distribuição de comprimentos das frases\n",
    "axes[0, 0].hist(pt_lengths, bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Distribuição do Comprimento das Frases (Português)', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Número de Palavras')\n",
    "axes[0, 0].set_ylabel('Frequência')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].hist(en_lengths, bins=10, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "axes[0, 1].set_title('Distribuição do Comprimento das Frases (Inglês)', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Número de Palavras')\n",
    "axes[0, 1].set_ylabel('Frequência')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Comparação de comprimentos\n",
    "axes[1, 0].scatter(pt_lengths, en_lengths, alpha=0.7, s=60)\n",
    "axes[1, 0].plot([0, max(max(pt_lengths), max(en_lengths))], [0, max(max(pt_lengths), max(en_lengths))], \n",
    "               'r--', alpha=0.5, label='Linha de referência')\n",
    "axes[1, 0].set_title('Comprimento: Português vs Inglês', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Palavras em Português')\n",
    "axes[1, 0].set_ylabel('Palavras em Inglês')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Tamanho dos vocabulários\n",
    "vocab_sizes = [pt_vocab.n_words, en_vocab.n_words]\n",
    "vocab_names = ['Português', 'Inglês']\n",
    "bars = axes[1, 1].bar(vocab_names, vocab_sizes, color=['skyblue', 'lightcoral'], edgecolor='black')\n",
    "axes[1, 1].set_title('Tamanho dos Vocabulários', fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Número de Palavras Únicas')\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "for bar, size in zip(bars, vocab_sizes):\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, \n",
    "                   str(size), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Estatísticas das frases:')\n",
    "print(f'Português - Média: {np.mean(pt_lengths):.1f} palavras, Max: {max(pt_lengths)}, Min: {min(pt_lengths)}')\n",
    "print(f'Inglês - Média: {np.mean(en_lengths):.1f} palavras, Max: {max(en_lengths)}, Min: {min(en_lengths)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Preparação dos Dados para Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(translation_pairs, pt_vocab, en_vocab, max_length=15):\n",
    "    \"\"\"Prepara os dados para treinamento com padding e tokens especiais\"\"\"\n",
    "    \n",
    "    data = []\n",
    "    skipped = 0\n",
    "    \n",
    "    for pt_sentence, en_sentence in translation_pairs:\n",
    "        # Converter frases para índices\n",
    "        pt_indices = pt_vocab.sentence_to_indices(pt_sentence)\n",
    "        en_indices = en_vocab.sentence_to_indices(en_sentence)\n",
    "        \n",
    "        # Verificar se as frases não são muito longas\n",
    "        if len(pt_indices) <= max_length - 2 and len(en_indices) <= max_length - 2:\n",
    "            # Adicionar padding para português (entrada do encoder)\n",
    "            pt_padded = pt_indices + [pt_vocab.word2idx['<PAD>']] * (max_length - len(pt_indices))\n",
    "            \n",
    "            # Adicionar <SOS> e <EOS> para inglês (entrada/saída do decoder)\n",
    "            en_input = [en_vocab.word2idx['<SOS>']] + en_indices + [en_vocab.word2idx['<EOS>']]\n",
    "            en_input += [en_vocab.word2idx['<PAD>']] * (max_length - len(en_input))\n",
    "            \n",
    "            # Target é o en_input deslocado (para teacher forcing)\n",
    "            en_target = en_indices + [en_vocab.word2idx['<EOS>']]\n",
    "            en_target += [en_vocab.word2idx['<PAD>']] * (max_length - len(en_target))\n",
    "            \n",
    "            data.append((pt_padded, en_input, en_target))\n",
    "        else:\n",
    "            skipped += 1\n",
    "    \n",
    "    print(f'Dados preparados: {len(data)} exemplos')\n",
    "    if skipped > 0:\n",
    "        print(f'Frases ignoradas por serem muito longas: {skipped}')\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Preparar dados de treinamento\n",
    "MAX_LENGTH = 15\n",
    "train_data = prepare_training_data(translation_pairs, pt_vocab, en_vocab, MAX_LENGTH)\n",
    "\n",
    "# Mostrar exemplo de dados preparados\n",
    "pt_example, en_input_example, en_target_example = train_data[0]\n",
    "print(f'\\nExemplo de dados preparados:')\n",
    "print(f'Entrada PT (índices): {pt_example}')\n",
    "print(f'Entrada EN (índices): {en_input_example}')\n",
    "print(f'Target EN (índices):  {en_target_example}')\n",
    "print(f'\\nConvertendo de volta para texto:')\n",
    "print(f'PT: \"{pt_vocab.indices_to_sentence(pt_example)}\"')\n",
    "print(f'EN input: \"{en_vocab.indices_to_sentence(en_input_example)}\"')\n",
    "print(f'EN target: \"{en_vocab.indices_to_sentence(en_target_example)}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset e DataLoader\n",
    "class TranslationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        pt_seq, en_input, en_target = self.data[idx]\n",
    "        return (\n",
    "            torch.tensor(pt_seq, dtype=torch.long),\n",
    "            torch.tensor(en_input, dtype=torch.long),\n",
    "            torch.tensor(en_target, dtype=torch.long)\n",
    "        )\n",
    "\n",
    "# Criar dataset e dataloader\n",
    "dataset = TranslationDataset(train_data)\n",
    "BATCH_SIZE = 8\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "print(f'Dataset criado:')\n",
    "print(f'- {len(dataset)} exemplos de treinamento')\n",
    "print(f'- Batch size: {BATCH_SIZE}')\n",
    "print(f'- {len(dataloader)} batches por época')\n",
    "\n",
    "# Testar um batch\n",
    "sample_batch = next(iter(dataloader))\n",
    "pt_batch, en_input_batch, en_target_batch = sample_batch\n",
    "print(f'\\nForma dos tensores em um batch:')\n",
    "print(f'PT input: {pt_batch.shape}')\n",
    "print(f'EN input: {en_input_batch.shape}')\n",
    "print(f'EN target: {en_target_batch.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Implementação do Modelo Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder LSTM que processa a sequência de entrada\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Camada de embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input_seq, hidden=None):\n",
    "        # input_seq: (batch_size, seq_len)\n",
    "        batch_size = input_seq.size(0)\n",
    "        \n",
    "        # Embedding\n",
    "        embedded = self.dropout(self.embedding(input_seq))\n",
    "        # embedded: (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # Passar pelo LSTM\n",
    "        if hidden is None:\n",
    "            outputs, (hidden_state, cell_state) = self.lstm(embedded)\n",
    "        else:\n",
    "            outputs, (hidden_state, cell_state) = self.lstm(embedded, hidden)\n",
    "        \n",
    "        # outputs: (batch_size, seq_len, hidden_dim)\n",
    "        # hidden_state, cell_state: (num_layers, batch_size, hidden_dim)\n",
    "        \n",
    "        return outputs, (hidden_state, cell_state)\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Inicializa o estado oculto com zeros\"\"\"\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device)\n",
    "        return (h0, c0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder LSTM que gera a sequência de saída\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Camada de embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Camada de saída\n",
    "        self.output_projection = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, input_token, hidden):\n",
    "        # input_token: (batch_size, 1)\n",
    "        # hidden: tuple of (hidden_state, cell_state)\n",
    "        \n",
    "        # Embedding\n",
    "        embedded = self.dropout(self.embedding(input_token))\n",
    "        # embedded: (batch_size, 1, embedding_dim)\n",
    "        \n",
    "        # Passar pelo LSTM\n",
    "        output, (hidden_state, cell_state) = self.lstm(embedded, hidden)\n",
    "        # output: (batch_size, 1, hidden_dim)\n",
    "        \n",
    "        # Projeção para vocabulário\n",
    "        prediction = self.output_projection(output)\n",
    "        # prediction: (batch_size, 1, vocab_size)\n",
    "        \n",
    "        return prediction, (hidden_state, cell_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    \"\"\"Modelo completo Encoder-Decoder\"\"\"\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "        # Verificar se as dimensões são compatíveis\n",
    "        assert encoder.hidden_dim == decoder.hidden_dim, \\\n",
    "            \"Encoder e Decoder devem ter a mesma dimensão oculta\"\n",
    "        assert encoder.num_layers == decoder.num_layers, \\\n",
    "            \"Encoder e Decoder devem ter o mesmo número de camadas\"\n",
    "    \n",
    "    def forward(self, source, target, teacher_forcing_ratio=0.5):\n",
    "        # source: (batch_size, source_len)\n",
    "        # target: (batch_size, target_len)\n",
    "        \n",
    "        batch_size = source.shape[0]\n",
    "        target_len = target.shape[1]\n",
    "        target_vocab_size = self.decoder.vocab_size\n",
    "        \n",
    "        # Tensor para armazenar as saídas do decoder\n",
    "        outputs = torch.zeros(batch_size, target_len, target_vocab_size).to(self.device)\n",
    "        \n",
    "        # Encoder: processar sequência de entrada\n",
    "        encoder_outputs, encoder_hidden = self.encoder(source)\n",
    "        \n",
    "        # Usar o estado final do encoder como estado inicial do decoder\n",
    "        decoder_hidden = encoder_hidden\n",
    "        \n",
    "        # Primeiro input do decoder é sempre <SOS>\n",
    "        decoder_input = target[:, 0].unsqueeze(1)  # (batch_size, 1)\n",
    "        \n",
    "        # Gerar sequência de saída\n",
    "        for t in range(1, target_len):\n",
    "            # Passar pelo decoder\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "            \n",
    "            # Armazenar output\n",
    "            outputs[:, t] = decoder_output.squeeze(1)\n",
    "            \n",
    "            # Decidir próximo input: teacher forcing ou própria predição\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            \n",
    "            if teacher_force:\n",
    "                # Usar ground truth\n",
    "                decoder_input = target[:, t].unsqueeze(1)\n",
    "            else:\n",
    "                # Usar própria predição\n",
    "                decoder_input = decoder_output.argmax(dim=2)\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def translate(self, source, max_length=20, sos_token=1, eos_token=2):\n",
    "        \"\"\"Função para tradução durante inferência (sem teacher forcing)\"\"\"\n",
    "        self.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_size = source.shape[0]\n",
    "            \n",
    "            # Encoder\n",
    "            encoder_outputs, encoder_hidden = self.encoder(source)\n",
    "            \n",
    "            # Inicializar decoder\n",
    "            decoder_hidden = encoder_hidden\n",
    "            decoder_input = torch.tensor([[sos_token]] * batch_size).to(self.device)\n",
    "            \n",
    "            # Armazenar tokens de saída\n",
    "            outputs = []\n",
    "            \n",
    "            for _ in range(max_length):\n",
    "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden)\n",
    "                predicted_token = decoder_output.argmax(dim=2)\n",
    "                \n",
    "                outputs.append(predicted_token.squeeze(1))\n",
    "                decoder_input = predicted_token\n",
    "                \n",
    "                # Parar se todos os exemplos do batch geraram <EOS>\n",
    "                if (predicted_token == eos_token).all():\n",
    "                    break\n",
    "            \n",
    "            # Concatenar outputs\n",
    "            if outputs:\n",
    "                outputs = torch.stack(outputs, dim=1)  # (batch_size, seq_len)\n",
    "            else:\n",
    "                outputs = torch.tensor([[eos_token]] * batch_size).to(self.device)\n",
    "            \n",
    "            return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Inicialização e Configuração do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hiperparâmetros do modelo\n",
    "EMBEDDING_DIM = 128\n",
    "HIDDEN_DIM = 256\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.1\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 200\n",
    "TEACHER_FORCING_RATIO = 0.5\n",
    "\n",
    "# Criar encoder e decoder\n",
    "encoder = Encoder(\n",
    "    vocab_size=pt_vocab.n_words,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    vocab_size=en_vocab.n_words,\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    hidden_dim=HIDDEN_DIM,\n",
    "    num_layers=NUM_LAYERS,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "# Criar modelo seq2seq\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "# Configurar otimização\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=en_vocab.word2idx['<PAD>'])\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.8)\n",
    "\n",
    "# Estatísticas do modelo\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'Vocabulário português: {pt_vocab.n_words} palavras')\n",
    "print(f'Vocabulário inglês: {en_vocab.n_words} palavras')\n",
    "print(f'Dimensão dos embeddings: {EMBEDDING_DIM}')\n",
    "print(f'Dimensão oculta: {HIDDEN_DIM}')\n",
    "print(f'Número de camadas: {NUM_LAYERS}')\n",
    "print(f'Total de parâmetros treináveis: {count_parameters(model):,}')\n",
    "print(f'Dispositivo: {device}')\n",
    "print(f'\\nHiperparâmetros de treinamento:')\n",
    "print(f'Taxa de aprendizado: {LEARNING_RATE}')\n",
    "print(f'Teacher forcing ratio: {TEACHER_FORCING_RATIO}')\n",
    "print(f'Épocas: {NUM_EPOCHS}')\n",
    "print(f'Batch size: {BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Função de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device, teacher_forcing_ratio):\n",
    "    \"\"\"Treina o modelo por uma época\"\"\"\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    num_batches = len(dataloader)\n",
    "    \n",
    "    for batch_idx, (source, target_input, target_output) in enumerate(dataloader):\n",
    "        # Mover dados para dispositivo\n",
    "        source = source.to(device)\n",
    "        target_input = target_input.to(device)\n",
    "        target_output = target_output.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(source, target_input, teacher_forcing_ratio)\n",
    "        \n",
    "        # Calcular loss (ignorar primeiro token que é sempre <SOS>)\n",
    "        output_dim = output.shape[-1]\n",
    "        \n",
    "        output = output[:, 1:].contiguous().view(-1, output_dim)\n",
    "        target_output = target_output[:, 1:].contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(output, target_output)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping para estabilidade\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    return epoch_loss / num_batches\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    \"\"\"Avalia o modelo no conjunto de dados\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for source, target_input, target_output in dataloader:\n",
    "            source = source.to(device)\n",
    "            target_input = target_input.to(device)\n",
    "            target_output = target_output.to(device)\n",
    "            \n",
    "            # Forward pass sem teacher forcing para avaliação mais realista\n",
    "            output = model(source, target_input, teacher_forcing_ratio=0)\n",
    "            \n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].contiguous().view(-1, output_dim)\n",
    "            target_output = target_output[:, 1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(output, target_output)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listas para armazenar métricas\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "learning_rates = []\n",
    "\n",
    "print('=== INICIANDO TREINAMENTO ===')\n",
    "print(f'Total de épocas: {NUM_EPOCHS}')\n",
    "print(f'Exemplos por época: {len(train_data)}')\n",
    "print(f'Batches por época: {len(dataloader)}')\n",
    "print()\n",
    "\n",
    "# Barra de progresso\n",
    "progress_bar = tqdm(range(NUM_EPOCHS), desc='Treinamento')\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience = 20\n",
    "patience_counter = 0\n",
    "\n",
    "for epoch in progress_bar:\n",
    "    # Treinar uma época\n",
    "    train_loss = train_epoch(model, dataloader, optimizer, criterion, device, TEACHER_FORCING_RATIO)\n",
    "    \n",
    "    # Avaliar no conjunto de treinamento (como validação)\n",
    "    val_loss = evaluate_model(model, dataloader, criterion, device)\n",
    "    \n",
    "    # Atualizar scheduler\n",
    "    scheduler.step()\n",
    "    current_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    # Armazenar métricas\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    learning_rates.append(current_lr)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Salvar melhor modelo\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Atualizar barra de progresso\n",
    "    progress_bar.set_postfix({\n",
    "        'Train Loss': f'{train_loss:.4f}',\n",
    "        'Val Loss': f'{val_loss:.4f}',\n",
    "        'LR': f'{current_lr:.2e}',\n",
    "        'Best': f'{best_loss:.4f}'\n",
    "    })\n",
    "    \n",
    "    # Log detalhado a cada 25 épocas\n",
    "    if (epoch + 1) % 25 == 0:\n",
    "        print(f'\\nÉpoca {epoch+1}/{NUM_EPOCHS}:')\n",
    "        print(f'  Train Loss: {train_loss:.4f}')\n",
    "        print(f'  Val Loss: {val_loss:.4f}')\n",
    "        print(f'  Learning Rate: {current_lr:.2e}')\n",
    "        print(f'  Melhor Loss: {best_loss:.4f}')\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f'\\nEarly stopping na época {epoch+1}')\n",
    "        break\n",
    "\n",
    "# Carregar melhor modelo\n",
    "model.load_state_dict(best_model_state)\n",
    "print(f'\\n=== TREINAMENTO CONCLUÍDO ===')\n",
    "print(f'Melhor perda de validação: {best_loss:.4f}')\n",
    "print(f'Épocas treinadas: {len(train_losses)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualização do Processo de Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotar métricas de treinamento\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "\n",
    "# Curvas de perda\n",
    "axes[0, 0].plot(epochs, train_losses, 'b-', label='Treinamento', linewidth=2)\n",
    "axes[0, 0].plot(epochs, val_losses, 'r-', label='Validação', linewidth=2)\n",
    "axes[0, 0].set_title('Curvas de Perda', fontweight='bold', fontsize=14)\n",
    "axes[0, 0].set_xlabel('Época')\n",
    "axes[0, 0].set_ylabel('Perda')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Últimas 50 épocas\n",
    "if len(train_losses) > 50:\n",
    "    start_idx = -50\n",
    "    recent_epochs = epochs[start_idx:]\n",
    "    axes[0, 1].plot(recent_epochs, train_losses[start_idx:], 'b-', label='Treinamento', linewidth=2)\n",
    "    axes[0, 1].plot(recent_epochs, val_losses[start_idx:], 'r-', label='Validação', linewidth=2)\n",
    "    axes[0, 1].set_title('Últimas 50 Épocas', fontweight='bold', fontsize=14)\n",
    "else:\n",
    "    axes[0, 1].plot(epochs, train_losses, 'b-', label='Treinamento', linewidth=2)\n",
    "    axes[0, 1].plot(epochs, val_losses, 'r-', label='Validação', linewidth=2)\n",
    "    axes[0, 1].set_title('Curvas de Perda (Zoom)', fontweight='bold', fontsize=14)\n",
    "    \n",
    "axes[0, 1].set_xlabel('Época')\n",
    "axes[0, 1].set_ylabel('Perda')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Taxa de aprendizado\n",
    "axes[1, 0].plot(epochs, learning_rates, 'g-', linewidth=2)\n",
    "axes[1, 0].set_title('Taxa de Aprendizado', fontweight='bold', fontsize=14)\n",
    "axes[1, 0].set_xlabel('Época')\n",
    "axes[1, 0].set_ylabel('Learning Rate')\n",
    "axes[1, 0].set_yscale('log')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Diferença entre train e val loss\n",
    "loss_diff = [abs(t - v) for t, v in zip(train_losses, val_losses)]\n",
    "axes[1, 1].plot(epochs, loss_diff, 'purple', linewidth=2)\n",
    "axes[1, 1].set_title('Diferença |Train Loss - Val Loss|', fontweight='bold', fontsize=14)\n",
    "axes[1, 1].set_xlabel('Época')\n",
    "axes[1, 1].set_ylabel('Diferença Absoluta')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Estatísticas finais\n",
    "print(f'Estatísticas do Treinamento:')\n",
    "print(f'  Perda final de treinamento: {train_losses[-1]:.4f}')\n",
    "print(f'  Perda final de validação: {val_losses[-1]:.4f}')\n",
    "print(f'  Melhor perda de validação: {min(val_losses):.4f}')\n",
    "print(f'  Redução da perda: {((train_losses[0] - train_losses[-1]) / train_losses[0] * 100):.1f}%')\n",
    "print(f'  Taxa de aprendizado final: {learning_rates[-1]:.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, pt_vocab, en_vocab, device, max_length=15):\n",
    "    \"\"\"Traduz uma frase do português para o inglês\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Preprocessar entrada\n",
    "        tokens = pt_vocab.sentence_to_indices(sentence.strip().lower())\n",
    "        \n",
    "        # Truncar se necessário\n",
    "        if len(tokens) > max_length:\n",
    "            tokens = tokens[:max_length]\n",
    "        \n",
    "        # Padding\n",
    "        tokens_padded = tokens + [pt_vocab.word2idx['<PAD>']] * (max_length - len(tokens))\n",
    "        source = torch.tensor(tokens_padded).unsqueeze(0).to(device)  # (1, max_length)\n",
    "        \n",
    "        # Traduzir usando o modelo\n",
    "        translation_indices = model.translate(\n",
    "            source, \n",
    "            max_length=max_length,\n",
    "            sos_token=en_vocab.word2idx['<SOS>'],\n",
    "            eos_token=en_vocab.word2idx['<EOS>']\n",
    "        )\n",
    "        \n",
    "        # Converter índices de volta para palavras\n",
    "        translation_indices = translation_indices.squeeze(0).cpu().numpy()\n",
    "        translated_words = []\n",
    "        \n",
    "        for idx in translation_indices:\n",
    "            word = en_vocab.idx2word[idx]\n",
    "            if word == '<EOS>':\n",
    "                break\n",
    "            if word != '<PAD>':\n",
    "                translated_words.append(word)\n",
    "        \n",
    "        return ' '.join(translated_words)\n",
    "\n",
    "# Testar traduções no conjunto de treinamento\n",
    "print('=== TESTE DE TRADUÇÕES (CONJUNTO DE TREINAMENTO) ===')\n",
    "print('Format: PT → Predição | Ground Truth')\n",
    "print()\n",
    "\n",
    "test_indices = [0, 1, 2, 3, 4, 10, 15, 20]  # Alguns exemplos específicos\n",
    "\n",
    "for i in test_indices:\n",
    "    if i < len(translation_pairs):\n",
    "        pt_sentence, en_ground_truth = translation_pairs[i]\n",
    "        prediction = translate_sentence(model, pt_sentence, pt_vocab, en_vocab, device)\n",
    "        \n",
    "        # Verificar se a tradução está correta\n",
    "        is_correct = prediction.strip().lower() == en_ground_truth.strip().lower()\n",
    "        status = \"✓\" if is_correct else \"✗\"\n",
    "        \n",
    "        print(f'\"{pt_sentence}\" → \"{prediction}\" | \"{en_ground_truth}\"')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste com frases novas (fora do conjunto de treinamento)\n",
    "print('=== TESTE COM FRASES NOVAS (GENERALIZAÇÃO) ===')\n",
    "print()\n",
    "\n",
    "new_sentences = [\n",
    "    'ele está feliz',\n",
    "    'ela gosta de música',\n",
    "    'nós somos amigos',\n",
    "    'eles estudam português',\n",
    "    'você come bem'\n",
    "]\n",
    "\n",
    "expected_translations = [\n",
    "    'he is happy', \n",
    "    'she likes music',\n",
    "    'we are friends',\n",
    "    'they study portuguese',\n",
    "    'you eat well'\n",
    "]\n",
    "\n",
    "print('Testando capacidade de generalização:')\n",
    "for pt_sentence, expected in zip(new_sentences, expected_translations):\n",
    "    prediction = translate_sentence(model, pt_sentence, pt_vocab, en_vocab, device)\n",
    "    print(f'PT: \"{pt_sentence}\"')\n",
    "    print(f'Predição: \"{prediction}\"')\n",
    "    print(f'Esperado: \"{expected}\"')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
