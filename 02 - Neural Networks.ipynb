{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34a9efcd",
   "metadata": {},
   "source": [
    "# Revisão: Redes Neurais com PyTorch e `torch.nn`\n",
    "\n",
    "Este notebook serve como uma revisão concisa do processo de construção e treinamento de uma rede neural para classificação de imagens usando a interface de alto nível do PyTorch. O objetivo é recapitular os componentes essenciais, assumindo familiaridade prévia com os conceitos.\n",
    "\n",
    "## Tópicos Abordados\n",
    "\n",
    "1.  **Datasets e DataLoaders**: Carregamento eficiente de dados.\n",
    "2.  **Definição do Modelo com `nn.Module`**: Estruturando a rede neural.\n",
    "3.  **Função de Custo e Otimizador**: Os componentes do aprendizado.\n",
    "4.  **Loop de Treinamento Completo**: O ciclo de treino e validação."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed484a2c",
   "metadata": {},
   "source": [
    "## 1. Datasets e DataLoaders\n",
    "\n",
    "O primeiro passo em qualquer pipeline de machine learning é o carregamento de dados. A biblioteca `torchvision` oferece acesso direto a datasets clássicos, como o MNIST. Para alimentar o modelo de forma eficiente, envolvemos o `Dataset` em um `DataLoader`, que automatiza a criação de lotes (*batches*), o embaralhamento dos dados e o carregamento em paralelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55502a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Definindo transformações para normalizar os dados\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # Média e desvio padrão do MNIST\n",
    "])\n",
    "\n",
    "# Baixando os datasets de treino e teste\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, transform=transform, download=True\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, transform=transform\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29716ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando os DataLoaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Verificando as dimensões de um lote\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"Formato do batch de imagens: {images.shape}\")\n",
    "print(f\"Formato do batch de rótulos: {labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfdf5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Função para exibir uma imagem\n",
    "def imshow(img):\n",
    "    # A normalização precisa ser revertida para a visualização correta\n",
    "    # Média = 0.1307, Desvio Padrão = 0.3081\n",
    "    img = img * 0.3081 + 0.1307 \n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.axis('off') # Remove os eixos\n",
    "    plt.show()\n",
    "\n",
    "# Pega um lote (batch) de imagens de treino\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Mostra as imagens em uma grade\n",
    "# O make_grid organiza o lote de imagens em uma única imagem-grade\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# Imprime os rótulos correspondentes\n",
    "print('Rótulos: ', ' '.join(f'{labels[j].item()}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ff8910",
   "metadata": {},
   "source": [
    "## 2. Definição do Modelo com `nn.Module`\n",
    "\n",
    "Uma rede neural é uma coleção de parâmetros aprendíveis (pesos e biases) organizados em camadas. Em PyTorch, qualquer modelo customizado é definido como uma classe que herda de `torch.nn.Module`.\n",
    "\n",
    "A camada fundamental para redes neurais densas (ou totalmente conectadas) é a `nn.Linear`, que aplica a transformação afim $y = xW^T + b$. A combinação de camadas lineares com funções de ativação não lineares, como a `ReLU`, permite que a rede aprenda relações complexas nos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11d5366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # O 'forward pass' define como os dados fluem através das camadas\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instanciando o modelo\n",
    "input_size = 28 * 28 # Imagens MNIST são 28x28\n",
    "hidden_size = 256\n",
    "num_classes = 10\n",
    "\n",
    "model = NeuralNetwork(input_size, hidden_size, num_classes)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5b5e8c",
   "metadata": {},
   "source": [
    "## 3. Função de Custo e Otimizador\n",
    "\n",
    "Para treinar a rede, precisamos de dois componentes-chave:\n",
    "\n",
    "1.  **Função de Custo (Loss Function)**: Mede a discrepância entre as previsões do modelo e os rótulos verdadeiros. Para classificação multiclasse, a `nn.CrossEntropyLoss` é a escolha padrão, pois combina `LogSoftmax` e `NLLLoss` de forma numericamente estável.\n",
    "\n",
    "2.  **Otimizador (Optimizer)**: Implementa um algoritmo (como Adam ou SGD) que atualiza os pesos do modelo na direção que minimiza a função de custo, com base nos gradientes calculados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3990ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Definindo a função de custo\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Definindo o otimizador\n",
    "# Passamos os parâmetros do modelo que ele deve otimizar\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f63f741",
   "metadata": {},
   "source": [
    "## 4. Loop de Treinamento Completo\n",
    "\n",
    "O treinamento consiste em iterar sobre o dataset por várias épocas. Em cada iteração (sobre um lote de dados), o processo é sempre o mesmo:\n",
    "\n",
    "1.  Zerar os gradientes acumulados (`optimizer.zero_grad()`).\n",
    "2.  Realizar o *forward pass*: passar os dados pelo modelo para obter as previsões.\n",
    "3.  Calcular a perda (custo) comparando as previsões com os rótulos.\n",
    "4.  Realizar o *backward pass*: calcular os gradientes da perda em relação a cada parâmetro (`loss.backward()`).\n",
    "5.  Atualizar os pesos do modelo (`optimizer.step()`).\n",
    "\n",
    "Após cada época de treino, é uma boa prática avaliar o desempenho do modelo no conjunto de validação/teste, lembrando de usar o contexto `torch.no_grad()` para desativar o cálculo de gradientes e acelerar a computação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825e552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Configurações do treinamento\n",
    "num_epochs = 5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Treinando no dispositivo: {device}\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # --- Fase de Treinamento ---\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    progress_bar = tqdm(train_loader, desc=f'Época {epoch+1}/{num_epochs} [Treino]')\n",
    "    \n",
    "    for images, labels in progress_bar:\n",
    "        # Achatando as imagens de [batch_size, 1, 28, 28] para [batch_size, 784]\n",
    "        images = images.reshape(-1, 28*28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 1. Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 2. Backward e otimização\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "        progress_bar.set_postfix({'Perda Treino': f'{loss.item():.4f}'})\n",
    "\n",
    "    # --- Fase de Validação ---\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.reshape(-1, 28*28).to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Fim da Época {epoch+1}: Perda Média de Treino: {train_loss / len(train_loader):.4f}, Acurácia no Teste: {accuracy:.2f}%')\n",
    "\n",
    "print(\"Treinamento concluído!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b3a853",
   "metadata": {},
   "source": [
    "## 5. Inferência com o Modelo Treinado\n",
    "\n",
    "Após o treinamento, o modelo está pronto para ser usado para fazer previsões em dados que nunca viu. O processo de inferência consiste em pegar uma ou mais amostras de dados, passá-las pelo modelo no modo de avaliação (`model.eval()`) e interpretar a saída para obter a predição final.\n",
    "\n",
    "Para um modelo de classificação, a saída do modelo são os *logits* (pontuações brutas) para cada classe. A classe com o maior logit é a predição do modelo. Vamos visualizar algumas previsões no conjunto de teste para ver nosso modelo em ação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f08137",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Coloca o modelo em modo de avaliação\n",
    "model.eval()\n",
    "\n",
    "# Pega um lote de dados do conjunto de teste\n",
    "images, labels = next(iter(test_loader))\n",
    "\n",
    "# Move os dados para o mesmo dispositivo do modelo\n",
    "images_to_show = images.to(device)\n",
    "labels_to_show = labels.to(device)\n",
    "\n",
    "# Faz a inferência\n",
    "with torch.no_grad():\n",
    "    # Achatamos a imagem para o formato esperado pelo modelo\n",
    "    outputs = model(images_to_show.reshape(-1, 28*28))\n",
    "\n",
    "# Obtém as classes previstas pegando o índice do maior logit\n",
    "_, predicted_labels = torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0029423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando as previsões\n",
    "fig = plt.figure(figsize=(15, 6))\n",
    "for i in range(10): # Visualizar 10 imagens\n",
    "    ax = fig.add_subplot(2, 5, i+1, xticks=[], yticks=[])\n",
    "    \n",
    "    # Move a imagem de volta para a CPU para visualização com matplotlib\n",
    "    img = images[i].cpu()\n",
    "    \n",
    "    # Desnormaliza a imagem para exibição (opcional, mas melhora visualização)\n",
    "    img = img * 0.3081 + 0.1307\n",
    "    \n",
    "    plt.imshow(img.squeeze(), cmap='gray') # .squeeze() para remover dimensão de cor\n",
    "    \n",
    "    pred = predicted_labels[i].item()\n",
    "    true_label = labels_to_show[i].item()\n",
    "    \n",
    "    # Define a cor do título com base se a previsão está correta\n",
    "    color = \"green\" if pred == true_label else \"red\"\n",
    "    \n",
    "    ax.set_title(f\"Previsto: {pred}\\nReal: {true_label}\", color=color)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
