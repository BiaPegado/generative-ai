{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8788d42",
   "metadata": {},
   "source": [
    "# LLMs com Hugging Face e LangChain\n",
    "\n",
    "## Conteúdo\n",
    "\n",
    "Neste notebook, exploraremos os fundamentos do trabalho com LLMs. Dividiremos nosso estudo em duas seções principais:\n",
    "\n",
    "* **Hugging Face**:\n",
    "    * Introdução à biblioteca `transformers`.\n",
    "    * Uso de **Tokenizadores** para pré-processar texto.\n",
    "    * Carregamento e utilização de **Modelos de Linguagem** pré-treinados para geração de texto.\n",
    "\n",
    "* **LangChain**:\n",
    "    * Introdução ao framework LangChain e ao conceito de **Runnables** com a LangChain Expression Language (LCEL).\n",
    "    * Como instanciar e interagir com **LLMs** de provedores externos (ex: OpenAI).\n",
    "    * Criação e formatação de **Prompts** e **Prompt Templates**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9ec0fc",
   "metadata": {},
   "source": [
    "# Hugging Face\n",
    "\n",
    "O Hugging Face é uma empresa e uma comunidade de código aberto que se tornou o ecossistema central para o desenvolvimento de modelos de Processamento de Linguagem Natural (PLN). A sua principal biblioteca, `transformers`, fornece uma API unificada para acessar uma vasta gama de modelos pré-treinados, enquanto o *Model Hub* serve como um repositório para compartilhar modelos, datasets e demonstrações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e3ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bfaf6c",
   "metadata": {},
   "source": [
    "## Tokenizadores\n",
    "\n",
    "O processo de tokenização consiste em converter uma sequência de texto (string) em uma sequência de tokens, que são subsequentemente mapeados para identificadores numéricos (IDs). Este passo é fundamental, pois os modelos de linguagem não operam sobre texto puro, mas sim sobre representações numéricas. Estratégias de tokenização sub-word, como Byte-Pair Encoding (BPE) ou WordPiece, são comuns, pois conseguem lidar com vocabulários extensos e palavras fora do vocabulário (Out-Of-Vocabulary - OOV) de forma eficiente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3693a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Carregando um tokenizador pré-treinado do Hugging Face Hub.\n",
    "# \"gpt2\" é um modelo causal (autoregressivo) da OpenAI.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e770a1d",
   "metadata": {},
   "source": [
    "Uma vez que o tokenizador é carregado, podemos utilizá-lo para codificar nosso texto de entrada. O resultado é um dicionário contendo, entre outras coisas, os `input_ids`, que são a representação numérica dos tokens, e a `attention_mask`, um tensor que indica ao modelo quais tokens devem ser considerados no cálculo da atenção (útil para processamento em lote com sentenças de tamanhos diferentes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975fc4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Artificial Intelligence is changing the world\"\n",
    "\n",
    "# Codificando o texto em IDs de input\n",
    "encoded_input = tokenizer(text)\n",
    "\n",
    "input_ids = encoded_input['input_ids']\n",
    "attention_mask = encoded_input['attention_mask']\n",
    "\n",
    "print(f\"Texto Original: {text}\")\n",
    "print(f\"Input IDs: {input_ids}\")\n",
    "print(f\"Attention Mask: {attention_mask}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d5a942",
   "metadata": {},
   "source": [
    "O processo inverso, a decodificação, converte a sequência de `input_ids` de volta para uma string legível. Isso é essencial para interpretar a saída gerada pelo modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0672a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decodificando os IDs de volta para texto\n",
    "decoded_text = tokenizer.decode(input_ids)\n",
    "\n",
    "print(f\"Texto Decodificado: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacd3fd1",
   "metadata": {},
   "source": [
    "## Modelos de Linguagem\n",
    "\n",
    "A biblioteca `transformers` permite carregar modelos pré-treinados com a mesma simplicidade dos tokenizadores. Para tarefas de geração de texto, como a que estamos explorando, utilizamos classes como `AutoModelForCausalLM`. \"Causal LM\" refere-se a modelos de linguagem causal, que preveem o próximo token em uma sequência de forma autorregressiva."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad86b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "# Carregando um modelo pré-treinado para linguagem causal (\"text generation\")\n",
    "# Este modelo corresponde ao tokenizador \"gpt2\" que carregamos anteriormente\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97852c52",
   "metadata": {},
   "source": [
    "Com o modelo e os inputs tokenizados em mãos, podemos realizar a inferência. O método `generate` é a principal interface para esta tarefa. Ele aceita uma variedade de parâmetros para controlar o processo de decodificação, como `max_length` (o comprimento máximo da sequência de saída) e `num_return_sequences` (o número de sequências independentes a serem geradas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3499153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# O método generate precisa de um tensor do PyTorch como entrada.\n",
    "import torch\n",
    "\n",
    "input_tensor = torch.tensor([input_ids])\n",
    "\n",
    "# Gerando a continuação do texto\n",
    "# Estamos pedindo ao modelo para gerar 50 tokens no total (input + output)\n",
    "output_sequences = model.generate(\n",
    "    input_tensor,\n",
    "    max_length=50,\n",
    "    num_return_sequences=1,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# A saída é uma lista de sequências de IDs\n",
    "generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Texto Gerado pelo Modelo:\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d2c5ec",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "\n",
    "LangChain é um framework projetado para simplificar o desenvolvimento de aplicações que utilizam LLMs. Ele fornece abstrações para componentes comuns, como modelos de linguagem, prompts e parsers de saída, e uma maneira declarativa de encadeá-los. O objetivo é facilitar a criação de aplicações complexas, como chatbots, sistemas de Resposta a Perguntas (Question Answering) e agentes autônomos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2161c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação das bibliotecas da LangChain e OpenAI\n",
    "# !pip install langchain langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0195bfe0",
   "metadata": {},
   "source": [
    "## O Conceito de Runnables e LCEL\n",
    "\n",
    "O coração do LangChain moderno é a LangChain Expression Language (LCEL). A LCEL fornece uma sintaxe declarativa para compor diferentes componentes. Qualquer objeto que segue o protocolo `Runnable` pode ser parte de uma cadeia LCEL. Este protocolo padroniza métodos como `invoke` (execução síncrona), `stream` (streaming da resposta) e `batch` (processamento em lote), unificando a interação com os componentes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed15e33d",
   "metadata": {},
   "source": [
    "## LLMs em LangChain\n",
    "\n",
    "LangChain oferece integrações com dezenas de provedores de modelos de linguagem. Para este exemplo, usaremos a integração com a OpenAI. É necessário possuir uma chave de API, que deve ser configurada como uma variável de ambiente para segurança."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6e1b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# É uma boa prática armazenar a chave de API como uma variável de ambiente.\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"SUA_CHAVE_API_AQUI\"\n",
    "\n",
    "# Instanciando o modelo de chat da OpenAI.\n",
    "# O parâmetro \"temperature\" controla a aleatoriedade da saída.\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb689c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322affef",
   "metadata": {},
   "source": [
    "Após a instanciação, o modelo se comporta como um `Runnable` e pode ser invocado diretamente com o método `invoke`. A entrada para um modelo de chat é tipicamente uma lista de mensagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec29ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Invocando o modelo com uma mensagem humana\n",
    "response = llm.invoke([HumanMessage(content=\"Explique o que é a computação quântica em uma frase.\")])\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bd3788",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ca5b45",
   "metadata": {},
   "source": [
    "## Prompts e Prompt Templates\n",
    "\n",
    "Um prompt é a entrada enviada a um Modelo de Linguagem para instruí-lo a realizar uma tarefa. A engenharia de prompts é a disciplina que estuda como construir prompts eficazes. Em LangChain, os `PromptTemplate`s são objetos que facilitam a criação de prompts de forma dinâmica a partir de entradas do usuário. Eles contêm uma string de template e um conjunto de variáveis de entrada, permitindo a reutilização e a estruturação de interações complexas com o LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7766fcdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Criando um template com uma variável de entrada \"topic\"\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Explique o que é {topic} em uma frase.\"\n",
    ")\n",
    "\n",
    "# Formatando o template com um valor para a variável\n",
    "formatted_prompt = prompt_template.format(topic=\"um buraco negro\")\n",
    "\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69489a1c",
   "metadata": {},
   "source": [
    "Para modelos de chat, que são otimizados para conversação, a entrada não é uma única string, mas uma lista de mensagens. Cada mensagem possui um conteúdo e um \"papel\" (role), que define quem a enviou. O `ChatPromptTemplate` é a ferramenta utilizada para estruturar essa lista de mensagens de forma dinâmica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b990c7",
   "metadata": {},
   "source": [
    "### Papéis das Mensagens em Modelos de Chat\n",
    "\n",
    "A estrutura de mensagens com papéis distintos é fundamental para controlar o comportamento dos modelos de chat. Os principais tipos são:\n",
    "\n",
    "* **SystemMessage**: Esta mensagem estabelece o contexto, as instruções de alto nível, a persona ou as restrições para o comportamento do LLM. Geralmente, é a primeira mensagem na sequência e atua como uma diretriz para todas as interações subsequentes na mesma conversa. O modelo é fortemente influenciado por ela para definir seu tom, estilo e objetivo.\n",
    "\n",
    "* **HumanMessage**: Representa a entrada do usuário final. É a mensagem que o LLM deve processar e à qual deve responder. Em uma aplicação, o conteúdo desta mensagem é tipicamente preenchido com a consulta do usuário.\n",
    "\n",
    "* **AIMessage**: Representa uma resposta previamente gerada pelo próprio modelo de IA. O uso de `AIMessage`s no prompt é uma técnica poderosa conhecida como *few-shot prompting*. Ao fornecer exemplos de interações (pares de `HumanMessage` e `AIMessage`), podemos \"ensinar\" o modelo em tempo de inferência sobre o formato, o estilo ou o tipo de resposta esperada, sem a necessidade de re-treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c52915e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Template de chat com duas mensagens: uma de sistema e uma humana\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um assistente de IA que explica conceitos científicos de forma simples e em apenas um parágrafo.\"),\n",
    "    (\"human\", \"Explique o que é {scientific_concept}.\")\n",
    "])\n",
    "\n",
    "# Formatando o template de chat\n",
    "formatted_chat_prompt = chat_template.format_messages(scientific_concept=\"a teoria da relatividade\")\n",
    "\n",
    "print(formatted_chat_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c133d1",
   "metadata": {},
   "source": [
    "A seguir, um exemplo mais avançado que utiliza uma `AIMessage` para fornecer um exemplo de resposta ao modelo (*few-shot*), guiando-o para que as saídas futuras sigam um formato específico (neste caso, \"Conceito: Explicação.\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ab9d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "# Exemplo de few-shot para guiar o formato da resposta\n",
    "few_shot_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um assistente que define termos técnicos. Responda sempre no formato 'Conceito: [explicação]'. Siga este formato estritamente.\"),\n",
    "    (\"human\", \"O que é uma Rede Neural?\"), # Exemplo de pergunta\n",
    "    (\"ai\", \"Rede Neural: Um modelo computacional inspirado na estrutura e funcionamento do cérebro humano, utilizado para tarefas de aprendizado de máquina.\"), # Exemplo de resposta no formato desejado\n",
    "    (\"human\", \"{user_query}\") # Pergunta real do usuário\n",
    "])\n",
    "\n",
    "# Formatando o template com uma nova consulta\n",
    "few_shot_prompt = few_shot_template.format_messages(user_query=\"O que é Processamento de Linguagem Natural?\")\n",
    "\n",
    "print(\"Prompt Formatado Enviado ao Modelo:\")\n",
    "print(few_shot_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0b51f7",
   "metadata": {},
   "source": [
    "## Output Parsers\n",
    "\n",
    "`OutputParser`s são componentes responsáveis por pegar a saída bruta de um LLM (geralmente uma string) e transformá-la em um formato mais estruturado e utilizável (ex: JSON, lista, etc.). O `StrOutputParser` é o mais simples: ele apenas garante que a saída seja uma string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee682c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Instanciando o parser de string\n",
    "string_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbd2dba",
   "metadata": {},
   "source": [
    "## Encadeando Componentes com LCEL\n",
    "\n",
    "Agora, podemos usar a sintaxe da LCEL para unir todos os componentes que vimos: `PromptTemplate`, `LLM` e `OutputParser`. O operador `|` passa o resultado do componente à esquerda como entrada para o componente à direita, criando um pipeline de processamento de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616bb38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando a cadeia (chain)\n",
    "# A entrada (um dicionário) vai para o `chat_template`.\n",
    "# A saída do template (um prompt formatado) vai para o `llm`.\n",
    "# A saída do llm (uma mensagem de IA) vai para o `string_parser`.\n",
    "# A saída do parser é o resultado final (uma string).\n",
    "chain = chat_template | llm | string_parser\n",
    "\n",
    "# Invocando a cadeia completa com uma única variável de entrada.\n",
    "result = chain.invoke({\"scientific_concept\": \"fissão nuclear\"})\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f16cfcc",
   "metadata": {},
   "source": [
    "Para ilustrar um parser mais complexo, vamos usar o `CommaSeparatedListOutputParser`, que instrui o modelo a retornar uma lista de itens separados por vírgula e, em seguida, converte essa string em uma lista Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee2389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "# Instanciando o parser de lista separada por vírgulas\n",
    "list_parser = CommaSeparatedListOutputParser()\n",
    "\n",
    "# O template agora inclui instruções de formatação fornecidas pelo parser\n",
    "list_prompt_template = PromptTemplate.from_template(\"Liste 5 exemplos de {category}. Sua resposta deve ser uma lista separada por vírgulas. Exemplo: foo, bar, baz\")\n",
    "\n",
    "# Criando a nova cadeia\n",
    "list_chain = list_prompt_template | llm | list_parser\n",
    "\n",
    "# Invocando a cadeia\n",
    "list_result = list_chain.invoke({\"category\": \"linguagens de programação\"})\n",
    "\n",
    "print(list_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f150f907",
   "metadata": {},
   "source": [
    "### Exemplo Prático: Tradução\n",
    "\n",
    "Vamos consolidar os conceitos anteriores em um exemplo prático. Criaremos uma cadeia de tradução que aceita como entrada o texto a ser traduzido, o idioma de origem e o idioma de destino. Isso demonstra a flexibilidade dos `PromptTemplates` para construir ferramentas reutilizáveis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a09a0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Definição do ChatPromptTemplate com múltiplas variáveis de entrada\n",
    "translation_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um tradutor poliglota altamente qualificado. Sua tarefa é traduzir o texto fornecido pelo usuário do idioma de origem para o idioma de destino com precisão.\"),\n",
    "    (\"human\", \"Por favor, traduza a seguinte frase de {source_language} para {target_language}: {text}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858d1a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Construção da cadeia de tradução usando LCEL\n",
    "translation_chain = translation_template | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f81d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Invocação da cadeia com os valores para as variáveis\n",
    "translation_result = translation_chain.invoke({\n",
    "    \"source_language\": \"português\",\n",
    "    \"target_language\": \"inglês\",\n",
    "    \"text\": \"Inteligência artificial generativa permite a criação de novos conteúdos de forma autônoma.\"\n",
    "})\n",
    "\n",
    "print(f\"Tradução: {translation_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897a89eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_result_2 = translation_chain.invoke({\n",
    "    \"source_language\": \"inglês\",\n",
    "    \"target_language\": \"espanhol\",\n",
    "    \"text\": \"Large Language Models are transforming the way we interact with technology.\"\n",
    "})\n",
    "\n",
    "print(f\"Tradução: {translation_result_2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306f1a92",
   "metadata": {},
   "source": [
    "### Geração de Saída Estruturada\n",
    "\n",
    "Em muitas aplicações, receber uma resposta em formato de string não é suficiente. É preferível que a saída do LLM siga um esquema predefinido e estruturado, como um objeto JSON. Isso elimina a necessidade de analisar strings de forma manual e propensa a erros, permitindo que a saída do modelo seja diretamente utilizada como um objeto de dados ou uma estrutura de dados na aplicação.\n",
    "\n",
    "LangChain facilita a geração de saídas estruturadas através da integração com a biblioteca **Pydantic**, que permite a definição de modelos de dados com tipagem estrita em Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ee157b",
   "metadata": {},
   "source": [
    "O processo geral consiste em:\n",
    "\n",
    "1.  **Definir um Schema**: Assim como antes, criamos uma classe que herda de `pydantic.BaseModel`, especificando os campos e os tipos de dados esperados.\n",
    "2.  **Configurar o Modelo**: Chamamos `llm.with_structured_output(schema)` para criar uma nova versão do modelo que sempre retornará objetos com a estrutura do `schema`.\n",
    "3.  **Encadear os Componentes**: A cadeia se torna mais simples: `prompt | modelo_estruturado`. Não há mais um parser explícito no final da cadeia.\n",
    "\n",
    "O resultado da invocação será uma instância do nosso modelo Pydantic, com os dados já validados e tipados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6563ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    \"\"\"Um modelo de dados que representa uma receita culinária.\"\"\"\n",
    "    name: str = Field(description=\"O nome da receita.\")\n",
    "    ingredients: List[str] = Field(description=\"Uma lista dos ingredientes necessários para a receita.\")\n",
    "    steps: List[str] = Field(description=\"Uma lista ordenada dos passos para preparar a receita.\")\n",
    "    preparation_time_minutes: int = Field(description=\"O tempo total de preparo em minutos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eeac136",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_llm = llm.with_structured_output(Recipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11c2705",
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Você é um chef de cozinha experiente que cria receitas simples e claras.\"),\n",
    "    (\"human\", \"Por favor, gere uma receita para: {query}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df4073ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_chain = recipe_prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f3ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invocando a cadeia com uma consulta\n",
    "user_query = \"Pizza de Mussarela\"\n",
    "structured_result = structured_chain.invoke({\"query\": user_query})\n",
    "\n",
    "# O resultado é um objeto Python, não uma string!\n",
    "print(f\"Tipo do Resultado: {type(structured_result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fa6c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podemos acessar seus atributos diretamente\n",
    "print(f\"Nome da Receita: {structured_result.name}\")\n",
    "print(f\"Tempo de Preparo: {structured_result.preparation_time_minutes} minutos\\n\")\n",
    "\n",
    "print(\"Ingredientes:\")\n",
    "for ingredient in structured_result.ingredients:\n",
    "    print(f\"- {ingredient}\")\n",
    "\n",
    "print(\"\\nPassos:\")\n",
    "for i, step in enumerate(structured_result.steps, 1):\n",
    "    print(f\"{i}. {step}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
