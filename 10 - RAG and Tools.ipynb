{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9d4e5e3",
   "metadata": {},
   "source": [
    "# Sumário\n",
    "\n",
    "1.  Introdução ao RAG (Retrieval-Augmented Generation)\n",
    "    * Ingestão de Dados\n",
    "        * Carregamento (Loading)\n",
    "        * Divisão (Splitting)\n",
    "        * Vetorização (Embedding)\n",
    "        * Armazenamento (Storing)\n",
    "        * Persistindo e Carregando o Banco de Dados Vetorial\n",
    "    * Recuperação e Geração\n",
    "2.  Ferramentas (Tools)\n",
    "    * Definindo Ferramentas Customizadas\n",
    "    * Conectando Ferramentas a um LLM\n",
    "    * Executando as Ferramentas e Gerando a Resposta Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc94ae54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install langchain langchain-openai langchain-community chromadb langchainhub python-dotenv beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d3fdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# É uma boa prática armazenar a chave de API como uma variável de ambiente.\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"SUA_CHAVE_API_AQUI\"\n",
    "\n",
    "# Instanciando o modelo de chat da OpenAI.\n",
    "# O parâmetro \"temperature\" controla a aleatoriedade da saída.\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaaa7d30",
   "metadata": {},
   "source": [
    "# 1. Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "Retrieval-Augmented Generation (RAG) é uma arquitetura que aprimora a capacidade de Modelos de Linguagem Grandes (LLMs) ao integrá-los com sistemas de recuperação de informação. Em vez de depender exclusivamente do conhecimento paramétrico internalizado durante o treinamento, o RAG permite que o modelo acesse e utilize informações de uma base de dados externa em tempo real para fundamentar suas respostas.\n",
    "\n",
    "O processo consiste em duas fases principais:\n",
    "\n",
    "1.  **Recuperação (Retrieval)**: Dado uma consulta (prompt) do usuário, o sistema busca em um grande corpus de documentos (por exemplo, um banco de dados vetorial) os trechos de texto mais relevantes para a consulta.\n",
    "2.  **Geração (Generation)**: O LLM recebe a consulta original juntamente com os documentos recuperados e utiliza essas informações como contexto para gerar uma resposta mais precisa, detalhada e factual."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a7aeec",
   "metadata": {},
   "source": [
    "## Ingestão de Dados\n",
    "\n",
    "A primeira etapa para construir um sistema RAG é o processamento e armazenamento da base de conhecimento que será utilizada. Este processo, conhecido como ingestão, compreende um pipeline de quatro passos: carregamento, divisão, vetorização e armazenamento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef85197",
   "metadata": {},
   "source": [
    "### Carregamento (Loading)\n",
    "\n",
    "A fase de carregamento consiste em importar os dados de suas fontes originais. Os documentos podem vir de diversos formatos, como arquivos de texto, PDFs, páginas da web, entre outros. A biblioteca LangChain oferece uma vasta gama de `DocumentLoaders` para facilitar essa tarefa.\n",
    "\n",
    "Neste exemplo, utilizaremos o `WebBaseLoader` para carregar o conteúdo diretamente de uma página da internet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223613a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# URL da página a ser carregada\n",
    "url = \"https://portal.imd.ufrn.br/portal/institucional/historico\"\n",
    "\n",
    "# Instancia o loader com a URL especificada\n",
    "loader = WebBaseLoader(\n",
    "    url,\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"texto-conteudo\")\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "# Carrega o conteúdo da página\n",
    "documents = loader.load()\n",
    "\n",
    "# Imprime uma prévia do conteúdo carregado\n",
    "print(documents[0].page_content.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b34a6bd",
   "metadata": {},
   "source": [
    "### Divisão (Splitting)\n",
    "\n",
    "LLMs possuem uma janela de contexto limitada, ou seja, um número máximo de tokens que podem processar de uma só vez. Documentos longos, como o conteúdo de uma página web, precisam ser divididos em pedaços menores (chunks) para que possam ser processados pelo modelo de embedding e, posteriormente, pelo LLM.\n",
    "\n",
    "A estratégia de divisão é crucial para a eficácia do RAG. Uma divisão inadequada pode separar contextos semanticamente coesos, prejudicando a qualidade da recuperação. O `RecursiveCharacterTextSplitter` é uma abordagem robusta que tenta manter parágrafos, sentenças e palavras juntos, recursivamente dividindo o texto por uma lista de separadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf18e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Inicializa o divisor de texto\n",
    "example_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,  # O tamanho máximo de cada chunk em caracteres\n",
    "    chunk_overlap=10   # O número de caracteres de sobreposição entre chunks adjacentes\n",
    ")\n",
    "\n",
    "text = \"Olá, este é um exemplo de texto que será dividido em chunks menores usando o RecursiveCharacterTextSplitter da biblioteca LangChain. O objetivo é garantir que cada chunk não exceda o tamanho máximo especificado, enquanto mantém uma sobreposição entre os chunks para preservar o contexto. Isso é especialmente útil quando se trabalha com modelos de linguagem que têm limites de tokens ou caracteres por entrada.\"\n",
    "\n",
    "example_text_splitter.split_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0794a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Inicializa o divisor de texto\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,  # O tamanho máximo de cada chunk em caracteres\n",
    "    chunk_overlap=50   # O número de caracteres de sobreposição entre chunks adjacentes\n",
    ")\n",
    "\n",
    "# Divide os documentos em chunks\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Número de documentos original: {len(documents)}\")\n",
    "print(f\"Número de chunks após a divisão: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67189cfe",
   "metadata": {},
   "source": [
    "### Vetorização (Embedding)\n",
    "\n",
    "Para que o sistema possa realizar buscas por similaridade semântica, os chunks de texto precisam ser convertidos em representações numéricas, conhecidas como embeddings. Um modelo de embedding é uma rede neural que mapeia um texto para um vetor de alta dimensão. Textos com significados semelhantes resultarão em vetores próximos no espaço vetorial.\n",
    "\n",
    "A distância entre dois vetores nesse espaço pode ser calculada por métricas como a similaridade de cosseno ou a distância Euclidiana.\n",
    "\n",
    "$$\\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74305f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Inicializa o modelo de embedding\n",
    "embedding_model = OpenAIEmbeddings()\n",
    "\n",
    "# Exemplo de como vetorizar um único texto\n",
    "example_embedding = embedding_model.embed_query(\"Quando o IMD foi criado?\")\n",
    "\n",
    "print(f\"Dimensão do vetor de embedding: {len(example_embedding)}\")\n",
    "print(f\"Primeiros 5 valores do embedding: {example_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9328602",
   "metadata": {},
   "source": [
    "### Armazenamento (Storing)\n",
    "\n",
    "Após a conversão dos chunks para vetores, eles são armazenados em um banco de dados especializado, chamado de *Vector Store* (ou banco de dados vetorial). Esses bancos de dados são otimizados para realizar buscas de similaridade de vetores de forma extremamente eficiente.\n",
    "\n",
    "Neste exemplo, usaremos o ChromaDB, um banco de dados vetorial de código aberto que pode ser executado em memória, facilitando a prototipação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c45d501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# Define o diretório onde o banco de dados vetorial será salvo\n",
    "persist_directory = 'vecdb_imd'\n",
    "\n",
    "# Cria o vectorstore a partir dos chunks, utilizando o modelo de embedding e especificando o diretório de persistência.\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embedding_model,\n",
    "    persist_directory=persist_directory\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d48099",
   "metadata": {},
   "source": [
    "### Persistindo e Carregando o Banco de Dados Vetorial\n",
    "\n",
    "O processo de ingestão de dados (carregamento, divisão e, principalmente, vetorização) pode ser computacionalmente caro e demorado, especialmente com grandes volumes de documentos. A persistência em disco é crucial.\n",
    "\n",
    "Uma vez que o `vectorstore` foi criado com um `persist_directory`, os dados já estão salvos. Em uma execução futura do código, podemos pular os passos de `load`, `split` e `from_documents` e carregar diretamente o banco de dados já existente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a241f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# É necessário fornecer a mesma função de embedding para que o Chroma saiba como vetorizar as buscas futuras\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=persist_directory,\n",
    "    embedding_function=embedding_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1a6a1d",
   "metadata": {},
   "source": [
    "## Recuperação e Geração\n",
    "\n",
    "Com o `vectorstore` criado (ou carregado do disco), podemos criar um \"retriever\". O retriever é um componente que, dada uma consulta, busca os documentos mais relevantes no `vectorstore`. Em seguida, construímos a cadeia RAG completa usando a LangChain Expression Language (LCEL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d6648d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6147eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Cria um retriever a partir do vectorstore\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",  # Tipo de busca (similaridade exata)\n",
    "    search_kwargs={\"k\": 2}     # Número de documentos a serem retornados\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608928ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"quando o imd começou com o bti?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9bec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define o template do prompt que será enviado ao LLM\n",
    "template = \"\"\"\n",
    "Responda a pergunta baseando-se somente no seguinte contexto:\n",
    "{context}\n",
    "\n",
    "Pergunta: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2098995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Inicializa o LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8752e1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Cria a cadeia RAG utilizando LCEL (LangChain Expression Language)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ddb141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Invoca a cadeia com uma pergunta sobre o conteúdo carregado\n",
    "query = \"o que aconteceu em 2015?\"\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920d92b0",
   "metadata": {},
   "source": [
    "# 2. Ferramentas (Tools)\n",
    "\n",
    "O conhecimento de um LLM é estático, limitado aos dados com os quais foi treinado. Ele não sabe a data atual, não pode acessar a internet em tempo real e não consegue interagir com APIs externas. As **Ferramentas (Tools)** são a solução para essa limitação.\n",
    "\n",
    "Uma ferramenta é, essencialmente, uma função que o LLM pode aprender a chamar. Ao disponibilizar um conjunto de ferramentas para um LLM, nós o capacitamos a:\n",
    "* Obter informações atualizadas (ex: notícias, previsão do tempo).\n",
    "* Executar cálculos complexos.\n",
    "* Interagir com qualquer sistema externo que possua uma API.\n",
    "\n",
    "O processo, conhecido como **Tool Calling** (ou *Function Calling*), funciona da seguinte forma: o LLM recebe o prompt do usuário e, em vez de gerar uma resposta textual, ele pode gerar uma instrução estruturada (JSON) para chamar uma ou mais ferramentas com os argumentos apropriados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17deaa51",
   "metadata": {},
   "source": [
    "## Definindo Ferramentas\n",
    "\n",
    "Podemos criar ferramentas a partir de qualquer função Python. O mais importante é fornecer uma *docstring* clara e descritiva para a função. O LLM utilizará essa documentação para entender o que a ferramenta faz, quais são seus parâmetros e quando ela deve ser utilizada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c65df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from datetime import date\n",
    "\n",
    "@tool\n",
    "def get_current_date() -> str:\n",
    "    \"\"\"Retorna a data de hoje no formato AAAA-MM-DD.\"\"\"\n",
    "    return str(date.today())\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiplica dois números, a e b.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# Lista de ferramentas que serão disponibilizadas para o LLM\n",
    "tools = [get_current_date, multiply]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e019c6",
   "metadata": {},
   "source": [
    "## Conectando Ferramentas a um LLM\n",
    "\n",
    "Uma vez definidas, as ferramentas precisam ser \"apresentadas\" ao LLM. Em LangChain, fazemos isso utilizando o método `.bind_tools()`. Esse método vincula as definições das ferramentas ao modelo, permitindo que ele as considere como possíveis ações ao processar um prompt.\n",
    "\n",
    "Quando invocamos o modelo com as ferramentas, a resposta não será uma string, mas sim uma mensagem especial (AIMessage) que pode conter `tool_calls`. Essas são as \"recomendações\" do LLM sobre quais funções executar para satisfazer o pedido do usuário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c72d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Vinculamos as ferramentas ao modelo\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Criamos uma lista de mensagens para a conversa\n",
    "# O prompt do usuário é a última mensagem\n",
    "messages = [\n",
    "    HumanMessage(content=\"Qual a data de hoje? E quanto é 12.3 multiplicado por 4.56?\")\n",
    "]\n",
    "\n",
    "# Invocamos o modelo\n",
    "ai_msg = llm_with_tools.invoke(messages)\n",
    "messages.append(ai_msg)\n",
    "\n",
    "# A resposta do LLM contém as chamadas de ferramenta que ele decidiu fazer\n",
    "print(ai_msg.tool_calls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47674854",
   "metadata": {},
   "source": [
    "## Executando as Ferramentas e Gerando a Resposta Final\n",
    "\n",
    "A saída do LLM no passo anterior é apenas uma instrução. Ela não executa as funções. Nosso código é responsável por:\n",
    "1.  Interpretar as `tool_calls` geradas pelo modelo.\n",
    "2.  Executar as funções Python correspondentes com os argumentos fornecidos.\n",
    "3.  Coletar os resultados de cada chamada.\n",
    "4.  Enviar esses resultados de volta para o LLM.\n",
    "\n",
    "Ao receber os resultados, o LLM pode finalmente sintetizar uma resposta final em linguagem natural para o usuário."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a75566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "# Dicionário para mapear nomes de ferramentas às suas funções\n",
    "tool_map = {tool.name: tool for tool in tools}\n",
    "\n",
    "# Iteramos sobre as chamadas de ferramenta recomendadas pelo LLM\n",
    "for tool_call in ai_msg.tool_calls:\n",
    "    # Obtemos a função a partir do seu nome\n",
    "    tool_to_call = tool_map[tool_call[\"name\"]]\n",
    "    # Invocamos a função com os argumentos fornecidos pelo LLM\n",
    "    observation = tool_to_call.invoke(tool_call[\"args\"])\n",
    "    # Adicionamos o resultado da execução da ferramenta ao histórico\n",
    "    messages.append(ToolMessage(content=str(observation), tool_call_id=tool_call[\"id\"]))\n",
    "\n",
    "# Agora, o histórico contém a pergunta do usuário, a decisão do LLM de usar as ferramentas,\n",
    "# e os resultados dessas ferramentas.\n",
    "# Invocamos o modelo novamente com o histórico completo.\n",
    "final_response = llm_with_tools.invoke(messages)\n",
    "\n",
    "print(final_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb83eace",
   "metadata": {},
   "source": [
    "### Simplificando o Ciclo de Execução de Ferramentas com LangGraph\n",
    "\n",
    "No processo manual que vimos, nós somos responsáveis por:\n",
    "1.  Receber as `tool_calls` do LLM.\n",
    "2.  Executar as funções correspondentes.\n",
    "3.  Formatar os resultados em `ToolMessage`.\n",
    "4.  Enviar tudo de volta ao LLM para a síntese final.\n",
    "\n",
    "Embora esse processo seja fundamental para o entendimento, ele pode se tornar repetitivo. A biblioteca **LangGraph** oferece abstrações de alto nível para construir agentes e aplicações complexas. Uma dessas abstrações é o `create_react_agent`, um \"grafo\" pré-construído que executa exatamente esse ciclo de raciocínio e ação para nós.\n",
    "\n",
    "Ele gerencia o estado da conversa (o histórico de mensagens), chama as ferramentas quando necessário e alimenta os resultados de volta ao modelo até que a tarefa seja concluída e uma resposta final seja gerada. Com isso, todo o nosso loop manual é substituído por uma única chamada `.invoke()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe04f392",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "# 1. Criamos o executor do agente com o modelo e as ferramentas\n",
    "agent = create_react_agent(\n",
    "    model=llm,  \n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edeac9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A estrutura espera uma chave \"messages\" contendo a lista\n",
    "response = agent.invoke({\n",
    "    \"messages\": [\n",
    "        HumanMessage(content=\"Qual a data de hoje? E quanto é 12.3 multiplicado por 4.56?\")\n",
    "    ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d081e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. O resultado contém o histórico completo da execução, incluindo as chamadas de ferramenta e a resposta final do LLM.\n",
    "print(\"--- Histórico Completo da Execução ---\")\n",
    "for message in response[\"messages\"]:\n",
    "    # O atributo .pretty_print() oferece uma visualização mais clara\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb532693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A última mensagem do assistente é a resposta consolidada\n",
    "final_message = response[\"messages\"][-1]\n",
    "print(final_message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9660ab90",
   "metadata": {},
   "source": [
    "# Exercícios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee4957d",
   "metadata": {},
   "source": [
    "## Exercício 1: Construindo seu Próprio Sistema RAG\n",
    "\n",
    "Implementar um pipeline RAG do início ao fim, utilizando um documento de sua escolha."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e9fe4d",
   "metadata": {},
   "source": [
    "## Exercício 2: Criando Ferramentas de Leitura e Escrita de Arquivos\n",
    "\n",
    "Implementar duas ferramentas para interagir com o sistema de arquivos local."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
